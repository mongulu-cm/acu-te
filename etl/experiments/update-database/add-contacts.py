# Generated by Selenium IDE
# npx @puppeteer/browsers install chrome@114.0.5735.133
import time
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
import os

def scroll_until_element_found(driver, css_selector, timeout=30, scroll_increment=200):
    end_time = time.time() + timeout
    while time.time() < end_time:
        try:
            element = WebDriverWait(driver, 1).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, css_selector))
            )
            if element:
                driver.execute_script("arguments[0].scrollIntoView();", element)
                return element
        except:
            pass

        # Scroll further down
        last_height = driver.execute_script("return document.body.scrollHeight")
        driver.execute_script(f"window.scrollTo(0, {last_height});")

        # Wait for the page to load
        time.sleep(1)

        # Check new scroll height
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            return None  # Reached bottom of the page

    return None  # Timeout

def format_text(text):
    return text.replace('\n', '').replace(' ', '').replace('\t', '')

chrome_options = Options()
chrome_options.add_argument('--disable-dev-shm-usage')
chrome_options.binary_location = '/Users/billmetangmo/Desktop/Projects/Platform/chrome/mac_arm-114.0.5735.133/chrome-mac-arm64/Google Chrome for Testing.app/Contents/MacOS/Google Chrome for Testing'
driver = webdriver.Chrome(options=chrome_options)
vars = {}

# Chargez le fichier CSV
chunk_size = 5
original_filepath = 'ref-rna-real-mars-2022-enriched-qualified.csv'
updated_filepath = 'ref-rna-real-mars-2022-enriched-qualified-contacts.csv'

file_already_exists = os.path.exists(updated_filepath)
if file_already_exists:
    updated_df = pd.read_csv(updated_filepath)
else:
    updated_df = pd.DataFrame()

csv_iterator = pd.read_csv(original_filepath, chunksize=chunk_size)

# Parcourir les URLs dans la colonne 'helloasso_url'
for chunk in csv_iterator:
    # Vérifiez si les colonnes "Email" et "Phone" existent, sinon créez-les
    if 'Email' not in chunk.columns:
        chunk['Email'] = None
    if 'Phone' not in chunk.columns:
        chunk['Phone'] = None

    chunk = pd.concat([chunk, updated_df]).drop_duplicates(keep=False)

    for index, row in chunk.iterrows():
        email = row.get('Email', None)
        phone = row.get('Phone', None)

        # Skip if 'Email' or 'Phone' are already filled
        if pd.notna(email) and pd.notna(phone):
            continue

        url = row['helloasso_url']
        if pd.isna(url):
            continue  # Skip if the URL is NaN

        driver.get(url)
        driver.set_window_size(1200, 828)

        # Faire défiler la page jusqu'à ce que l'élément avec le sélecteur CSS ".Info-Email .HaButton" soit visible
        element1 = scroll_until_element_found(driver, ".Info-Email .HaButton")

        if element1 is None:
            print("Reached bottom of the page, Email not found.")
        elif element1:
            wait = WebDriverWait(driver, 10)
            element1 = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.Info-Email .HaButton')))
            driver.execute_script("arguments[0].click();", element1)
            time.sleep(3)

        # Faire défiler la page jusqu'à ce que l'élément avec le sélecteur CSS ".Info--Field > .HaButton" soit visible
        element2 = scroll_until_element_found(driver, ".Info--Field > .HaButton")

        if element2 is None:
            print("Reached bottom of the page, Phone not found.")
        elif element2:
            wait = WebDriverWait(driver, 10)
            element2 = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.Info--Field > .HaButton')))
            driver.execute_script("arguments[0].click();", element2)
            time.sleep(3)

        html_content = driver.page_source
        soup = BeautifulSoup(html_content, 'html.parser')

        # Trouver l'email et le numéro de téléphone
        email_element = soup.find("div", {"class": "Info Info-Email"})
        email = format_text(email_element.find("p", {"class": "Info--Text"}).text) if email_element else None

        phone_element = soup.find("div", {"class": "Info Info-Phone"})
        phone = format_text(phone_element.find("p", {"class": "Info--Text"}).text) if phone_element else None

        # Imprimer les résultats
        print(f"Email: {email}, Phone: {phone}")
        chunk.at[index, 'Email'] = email
        chunk.at[index, 'Phone'] = phone
        print(chunk)

    if not file_already_exists:
        chunk.to_csv(updated_filepath, mode='w', header=True, index=False)
        file_already_exists = True
    else:
        chunk.to_csv(updated_filepath, mode='a', header=False, index=False)
